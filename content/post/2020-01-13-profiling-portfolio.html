---
title: Profiling Portfolio
author: Mauro Camara Escudero
date: '2020-01-13'
slug: profiling-portfolio
categories:
  - R
  - profiling
tags:
  - profiling
  - gaussian-processes
keywords:
  - tech
header-includes:
  - \usepackage{amsmath}
---

<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/jquery/jquery.min.js"></script>
<script src="/rmarkdown-libs/d3/d3.min.js"></script>
<link href="/rmarkdown-libs/profvis/profvis.css" rel="stylesheet" />
<script src="/rmarkdown-libs/profvis/profvis.js"></script>
<link href="/rmarkdown-libs/highlight/textmate.css" rel="stylesheet" />
<script src="/rmarkdown-libs/highlight/highlight.js"></script>
<script src="/rmarkdown-libs/profvis-binding/profvis.js"></script>


<div id="implementations" class="section level2">
<h2>Implementations</h2>
<div id="data-generation-and-problem-settings" class="section level3">
<h3>Data Generation and Problem Settings</h3>
<p>Import the necessary packages. The library <code>provfis</code> is a stochastic profiler and can be used to profile our code.</p>
<pre class="r"><code>library(profvis)
library(tidyverse)
library(MASS)
library(microbenchmark)</code></pre>
<p>First, let’s decide some parameters and settings such as the amount of training and testing data.</p>
<pre class="r"><code># Number of data points for plotting &amp; Bandwidth squared of the RBF kernel
ntest &lt;- 500
# Characteristic Length-scale for kernel
sigmasq &lt;- 1.0
# Number of training points, standard deviation of additive noise
ntrain &lt;- 10
sigma_n &lt;- 0.5
# Define number of samples for prior gp mvn and posterior gp mvn to take
nprior_samples &lt;- 5
npost_samples &lt;- 5</code></pre>
<p>Define some helper functions. In particular, the kernel function is a squared exponential. The bandwidth is computed as the median of all pairwise distances. We also define a matrix that applies the squared exponential to rows of two matrices, i.e. it computes the kernel matrix <span class="math inline">\(K(X, Y)\)</span>. for two matrices <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Finally, we also have a regression function which represents the true structure of the data. This is just a quintic polynomial for simplicity, but can be replaced by more complicated functions.</p>
<pre class="r"><code>squared_exponential &lt;- function(x, c, sigmasq){
  return(exp(-0.5*sum((x - c)^2) / sigmasq))
}
kernel_matrix &lt;- function(X, Xstar, sigmasq){
  # compute the kernel matrix
  K &lt;- apply(
    X=Xstar,
    MARGIN=1, 
    FUN=function(xstar_row) apply(
      X=X, 
      MARGIN=1, 
      FUN=squared_exponential, 
      xstar_row,
      sigmasq
      )
    )
  return(K)
}
regression_function &lt;- function(x){
    val &lt;- (x+5)*(x+2)*(x)*(x-4)*(x-3)/10 + 2
  return(val)
}</code></pre>
<p>Now let’s actually generate some data</p>
<pre class="r"><code>set.seed(12345)
# training data
xtrain &lt;- matrix(runif(ntrain, min=-5, max=5))
ytrain &lt;- regression_function(xtrain) + matrix(rnorm(ntrain, sd=sigma_n))
# testing data
xtest &lt;- matrix(seq(-5,5, len=ntest))</code></pre>
</div>
<div id="naive-vectorized-implementation" class="section level3">
<h3>Naive Vectorized Implementation</h3>
<p>The first implementation that we look at is naive in the sense that it basically blindly copies the operations. This means that we invert <span class="math inline">\(K + \sigma_n^2 I\)</span> directly.</p>
<pre class="r"><code>source(&quot;gp_naive.R&quot;, keep.source = TRUE)
profvis(result &lt;- gp_naive(xtrain, ytrain, xtest, sigma_n, sigmasq))</code></pre>
<div id="htmlwidget-1" style="width:100%;height:600px;" class="profvis html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"message":{"prof":{"time":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4,5,5,5,5,5,5,6,6,6,6,6,6,7,7,7,7,7,8,8,8,8,8,9,9,9,9,9,9,9,10,10,10,10,10,11,11,11,11,11,12,12,12,12,12,13,13,13,13,13,14,14,14,14,14,15,15,15,15,15,15,16,16,16,16,16,17,17,17,17,17,18,18,18,18,18,18,18,19,19,19,19,19,20,20,20,20,20,21,21,21,21,21,21,22,22,22,22,22,23,23,23,23,23,24,24,24,24,24,25,25,25,25,25,25,25,26,26,26,26,26,26,27,27,27,27,27,28,28,28,28,28,29,29,29,29,29,29,29,30,30,30,30,30,30,31,31,31,31,31,32,32,32,32,32,33,33,33,33,33,34,34,34,34,34,34,35,35,35,35,35,35,36,36,36,36,36,36,37,37,37,37,37],"depth":[30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,8,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1],"label":["cmpCallSymFun","cmpCall","cmp","cmpBuiltinArgs","h","tryInline","cmpCall","cmp","cmpPrim2","h","tryInline","cmpCall","cmp","cmpSymbolAssign","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","genCode","cmpfun","doTryCatch","tryCatchOne","tryCatchList","tryCatch","compiler:::tryCmpfun","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","FUN","apply","FUN","apply","kernel_matrix","gp_naive","<GC>","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","sum","FUN","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","lengths","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","sum","FUN","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","FUN","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","sum","FUN","apply","FUN","apply","kernel_matrix","gp_naive","FUN","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","<GC>","FUN","apply","FUN","apply","kernel_matrix","gp_naive","FUN","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","FUN","apply","FUN","apply","kernel_matrix","gp_naive","FUN","apply","FUN","apply","kernel_matrix","gp_naive","unlist","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive"],"filenum":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,null,2,null,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,null,null,2,null,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,null,null,2,null,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,null,2,null,null,null,null,null,2,null,null,null,null,null,2,null,null,null,null,2,null],"linenum":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,4,null,null,null,null,5,null,null,null,null,5,null,null,null,null,null,5,null,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,null,null,5,null,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,null,null,5,null,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,null,5,null,null,null,null,null,5,null,null,null,null,null,5,null,null,null,null,5,null],"memalloc":[15.2993850708008,15.2993850708008,15.2993850708008,15.2993850708008,15.2993850708008,15.2993850708008,15.2993850708008,15.2993850708008,15.2993850708008,15.2993850708008,15.2993850708008,15.2993850708008,15.2993850708008,15.2993850708008,15.2993850708008,15.2993850708008,15.2993850708008,15.2993850708008,15.2993850708008,15.2993850708008,15.2993850708008,15.2993850708008,15.2993850708008,15.2993850708008,15.2993850708008,15.2993850708008,15.2993850708008,15.2993850708008,15.2993850708008,15.2993850708008,15.6189041137695,15.6189041137695,15.6189041137695,15.6189041137695,15.6189041137695,16.3918991088867,16.3918991088867,16.3918991088867,16.3918991088867,16.3918991088867,16.7698822021484,16.7698822021484,16.7698822021484,16.7698822021484,16.7698822021484,17.3166198730469,17.3166198730469,17.3166198730469,17.3166198730469,17.3166198730469,17.3166198730469,17.6640014648438,17.6640014648438,17.6640014648438,17.6640014648438,17.6640014648438,17.6640014648438,13.0135803222656,13.0135803222656,13.0135803222656,13.0135803222656,13.0135803222656,13.5093231201172,13.5093231201172,13.5093231201172,13.5093231201172,13.5093231201172,14.210823059082,14.210823059082,14.210823059082,14.210823059082,14.210823059082,14.210823059082,14.210823059082,14.6850128173828,14.6850128173828,14.6850128173828,14.6850128173828,14.6850128173828,12.8679275512695,12.8679275512695,12.8679275512695,12.8679275512695,12.8679275512695,13.3709487915039,13.3709487915039,13.3709487915039,13.3709487915039,13.3709487915039,14.1113128662109,14.1113128662109,14.1113128662109,14.1113128662109,14.1113128662109,14.5677261352539,14.5677261352539,14.5677261352539,14.5677261352539,14.5677261352539,15.0893478393555,15.0893478393555,15.0893478393555,15.0893478393555,15.0893478393555,15.0893478393555,12.999641418457,12.999641418457,12.999641418457,12.999641418457,12.999641418457,13.7259750366211,13.7259750366211,13.7259750366211,13.7259750366211,13.7259750366211,14.1921997070312,14.1921997070312,14.1921997070312,14.1921997070312,14.1921997070312,14.1921997070312,14.1921997070312,14.9045333862305,14.9045333862305,14.9045333862305,14.9045333862305,14.9045333862305,15.3855514526367,15.3855514526367,15.3855514526367,15.3855514526367,15.3855514526367,13.7150726318359,13.7150726318359,13.7150726318359,13.7150726318359,13.7150726318359,13.7150726318359,14.2271728515625,14.2271728515625,14.2271728515625,14.2271728515625,14.2271728515625,14.9552459716797,14.9552459716797,14.9552459716797,14.9552459716797,14.9552459716797,15.4285507202148,15.4285507202148,15.4285507202148,15.4285507202148,15.4285507202148,13.7446594238281,13.7446594238281,13.7446594238281,13.7446594238281,13.7446594238281,13.7446594238281,13.7446594238281,14.2551040649414,14.2551040649414,14.2551040649414,14.2551040649414,14.2551040649414,14.2551040649414,14.9851913452148,14.9851913452148,14.9851913452148,14.9851913452148,14.9851913452148,15.4584045410156,15.4584045410156,15.4584045410156,15.4584045410156,15.4584045410156,16.074089050293,16.074089050293,16.074089050293,16.074089050293,16.074089050293,16.074089050293,16.074089050293,14.1712875366211,14.1712875366211,14.1712875366211,14.1712875366211,14.1712875366211,14.1712875366211,14.9288864135742,14.9288864135742,14.9288864135742,14.9288864135742,14.9288864135742,15.3903579711914,15.3903579711914,15.3903579711914,15.3903579711914,15.3903579711914,16.0968704223633,16.0968704223633,16.0968704223633,16.0968704223633,16.0968704223633,14.1567611694336,14.1567611694336,14.1567611694336,14.1567611694336,14.1567611694336,14.1567611694336,14.9209899902344,14.9209899902344,14.9209899902344,14.9209899902344,14.9209899902344,14.9209899902344,15.4065475463867,15.4065475463867,15.4065475463867,15.4065475463867,15.4065475463867,15.4065475463867,16.1268157958984,16.1268157958984,16.1268157958984,16.1268157958984,16.1268157958984],"meminc":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.31951904296875,0,0,0,0,0.772994995117188,0,0,0,0,0.377983093261719,0,0,0,0,0,0,0,0,0,0,0.347381591796875,0,0,0,0,0,-4.65042114257812,0,0,0,0,0.495742797851562,0,0,0,0,0.701499938964844,0,0,0,0,0,0,0.474189758300781,0,0,0,0,-1.81708526611328,0,0,0,0,0.503021240234375,0,0,0,0,0.740364074707031,0,0,0,0,0.456413269042969,0,0,0,0,0.521621704101562,0,0,0,0,0,-2.08970642089844,0,0,0,0,0.726333618164062,0,0,0,0,0,0,0,0,0,0,0,0.712333679199219,0,0,0,0,0.48101806640625,0,0,0,0,0,0,0,0,0,0,0.512100219726562,0,0,0,0,0.728073120117188,0,0,0,0,0.473304748535156,0,0,0,0,-1.68389129638672,0,0,0,0,0,0,0,0,0,0,0,0,0.730087280273438,0,0,0,0,0.473213195800781,0,0,0,0,0.615684509277344,0,0,0,0,0,0,0,0,0,0,0,0,0.757598876953125,0,0,0,0,0.461471557617188,0,0,0,0,0.706512451171875,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.485557556152344,0,0,0,0,0,0.720268249511719,0,0,0,0],"filename":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,null,"gp_naive.R",null,null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,null,null,"gp_naive.R",null,null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,null,null,"gp_naive.R",null,null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,null,"gp_naive.R",null,null,null,null,null,"gp_naive.R",null,null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null]},"interval":10,"files":[{"filename":"gp_naive.R","content":"gp_naive <- function(xtrain, ytrain, xtest, sigma_n, sigmasq){\n  # Find kernel matrices in vectorized form\n  K  <-  kernel_matrix(xtrain, xtrain, sigmasq) \n  Ks <-  kernel_matrix(xtest,  xtrain, sigmasq)\n  Kss <- kernel_matrix(xtest,  xtest, sigmasq)\n  # Find the inverse of K + sigma_n^2I directly\n  inverse <- solve(K + sigma_n^2 * diag(ntrain))\n  # GP mean\n  gpmean <- Ks %*% (inverse %*% ytrain)\n  # GP variance-covariance matrix\n  gpvcov <- Kss - Ks %*% inverse %*% t(Ks)\n  return(list(gpmean, gpvcov))\n}\n","normpath":"/home/mauro/Documents/University/StatisticalComputingPortfolioWebsite/content/post/gp_naive.R"}],"prof_output":"/tmp/RtmpOLBSfd/file6d6124612d3b.prof","highlight":{"output":["^output\\$"],"gc":["^<GC>$"],"stacktrace":["^\\.\\.stacktraceo(n|ff)\\.\\.$"]},"split":"h"}},"evals":[],"jsHooks":[]}</script>
</div>
<div id="online-non-vectorized-cholesky-implementation" class="section level3">
<h3>Online Non-Vectorized Cholesky Implementation</h3>
<p>This implementation can also be used <em>online</em>. It is recommended in the “Gaussian Processes for Machine Learning” book.</p>
<pre class="r"><code>source(&quot;gp_online.R&quot;, keep.source = TRUE)
profvis(result_online &lt;- gp_online(xtrain, ytrain, xtest, sigma_n, sigmasq))</code></pre>
<div id="htmlwidget-2" style="width:100%;height:600px;" class="profvis html-widget"></div>
<script type="application/json" data-for="htmlwidget-2">{"x":{"message":{"prof":{"time":[1,1,1,1,2,2,2,3,3,3,3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,6,6,6,6,7,7,7,7,8,8,8,9,9,9,9,9,9,10,10,10,10,10,10],"depth":[4,3,2,1,3,2,1,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,4,3,2,1,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1],"label":["as.matrix","forwardsolve","crossprod","gp_online","gpvar[row_ix] <- 1.0 - crossprod(forwardsolve(t(L), kstar))  ## as SE(x*, x*) = 1.0","crossprod","gp_online","apply","FUN","apply","kernel_matrix","gp_online","unlist","apply","FUN","apply","kernel_matrix","gp_online","aperm","apply","FUN","apply","kernel_matrix","gp_online","<GC>","length","apply","FUN","apply","kernel_matrix","gp_online","FUN","apply","kernel_matrix","gp_online","forwardsolve","crossprod","gp_online","aperm","apply","FUN","apply","kernel_matrix","gp_online","match.fun","apply","FUN","apply","kernel_matrix","gp_online"],"filenum":[null,1,1,null,1,1,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,1,null,1,1,null,null,null,null,null,1,null,null,null,null,null,1,null],"linenum":[null,17,17,null,17,17,null,null,null,null,13,null,null,null,null,null,13,null,null,null,null,null,13,null,null,null,null,null,null,13,null,null,null,13,null,17,17,null,null,null,null,null,13,null,null,null,null,null,13,null],"memalloc":[15.5306243896484,15.5306243896484,15.5306243896484,15.5306243896484,15.8862686157227,15.8862686157227,15.8862686157227,16.3823623657227,16.3823623657227,16.3823623657227,16.3823623657227,16.3823623657227,16.6551666259766,16.6551666259766,16.6551666259766,16.6551666259766,16.6551666259766,16.6551666259766,17.011833190918,17.011833190918,17.011833190918,17.011833190918,17.011833190918,17.011833190918,17.2963562011719,17.2963562011719,17.2963562011719,17.2963562011719,17.2963562011719,17.2963562011719,17.2963562011719,15.2697372436523,15.2697372436523,15.2697372436523,15.2697372436523,15.625244140625,15.625244140625,15.625244140625,16.1431350708008,16.1431350708008,16.1431350708008,16.1431350708008,16.1431350708008,16.1431350708008,16.4917984008789,16.4917984008789,16.4917984008789,16.4917984008789,16.4917984008789,16.4917984008789],"meminc":[0,0,0,0,0.355644226074219,0,0,0.49609375,0,0,0,0,0.272804260253906,0,0,0,0,0,0.356666564941406,0,0,0,0,0,0.284523010253906,0,0,0,0,0,0,-2.02661895751953,0,0,0,0.355506896972656,0,0,0.517890930175781,0,0,0,0,0,0,0,0,0,0,0],"filename":[null,"gp_online.R","gp_online.R",null,"gp_online.R","gp_online.R",null,null,null,null,"gp_online.R",null,null,null,null,null,"gp_online.R",null,null,null,null,null,"gp_online.R",null,null,null,null,null,null,"gp_online.R",null,null,null,"gp_online.R",null,"gp_online.R","gp_online.R",null,null,null,null,null,"gp_online.R",null,null,null,null,null,"gp_online.R",null]},"interval":10,"files":[{"filename":"gp_online.R","content":"gp_online <- function(xtrain, ytrain, xtest, sigma_n, sigmasq){\n  # Find kernel matrix\n  K <- kernel_matrix(xtrain, xtrain, sigmasq) \n  # Cholesky factor for K + sigma^2 * I\n  L <- chol(K + sigma_n^2*diag(ntrain))  ## Upper triangular\n  alpha <- backsolve(L, forwardsolve(t(L), ytrain))\n  # Allocate memory first\n  gpmean <- rep(0, ntest)\n  gpvar  <- rep(0, ntest)\n  # Loop through all test rows of do online regression\n  for (row_ix in 1:nrow(xtest)){\n    # Find kernel evaluation against all training points\n    kstar <- kernel_matrix(matrix(xtest[row_ix, ]), xtrain, sigmasq)\n    # GP mean for current test point\n    gpmean[row_ix] <- t(kstar) %*% alpha\n    # GP variance for current test point\n    gpvar[row_ix] <- 1.0 - crossprod(forwardsolve(t(L), kstar))  ## as SE(x*, x*) = 1.0\n  }\n  return(list(gpmean, gpvar))\n}","normpath":"/home/mauro/Documents/University/StatisticalComputingPortfolioWebsite/content/post/gp_online.R"}],"prof_output":"/tmp/RtmpOLBSfd/file6d61270d39a5.prof","highlight":{"output":["^output\\$"],"gc":["^<GC>$"],"stacktrace":["^\\.\\.stacktraceo(n|ff)\\.\\.$"]},"split":"h"}},"evals":[],"jsHooks":[]}</script>
</div>
<div id="online-vectorized-kernel-cholesky-implementation" class="section level3">
<h3>Online Vectorized-Kernel Cholesky Implementation</h3>
<p>We can see that most of the time is spent computing the kernel matrix. We can therefore find a faster way to compute it as follows</p>
<pre class="r"><code>kernel_matrix_vectorized &lt;- function(X, sigmasq, Y=NULL){
  if (is.null(Y)){
    Y &lt;- X
  }
  n &lt;- nrow(X)
  m &lt;- nrow(Y)
  # Find three matrices above
  Xnorm &lt;- matrix(apply(X^2, 1, sum), n, m)
  Ynorm &lt;- matrix(apply(Y^2, 1, sum), n, m, byrow=TRUE)
  XY &lt;- tcrossprod(X, Y)
  return(exp(-(Xnorm - 2*XY + Ynorm) / (2*sigmasq)))
}</code></pre>
<p>using this, we get</p>
<pre class="r"><code>source(&quot;gp_online_vect.R&quot;, keep.source = TRUE)
profvis(gp_online_vect(xtrain, ytrain, xtest, sigma_n, sigmasq))</code></pre>
<div id="htmlwidget-3" style="width:100%;height:600px;" class="profvis html-widget"></div>
<script type="application/json" data-for="htmlwidget-3">{"x":{"message":{"prof":{"time":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,4,4,4,4],"depth":[35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,4,3,2,1,4,3,2,1],"label":["%in%","findCenvVar","findVar","cmpSym","cmp","genCode","cb$putconst","cmpCallArgs","cmpCallSymFun","cmpCall","cmp","genCode","cb$putconst","cmpCallArgs","cmpCallSymFun","cmpCall","cmp","cmpSymbolAssign","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","genCode","cmpfun","doTryCatch","tryCatchOne","tryCatchList","tryCatch","compiler:::tryCmpfun","kernel_matrix_vectorized","gp_online_vect","match.fun","apply","matrix","kernel_matrix_vectorized","gp_online_vect","apply","matrix","kernel_matrix_vectorized","gp_online_vect","apply","matrix","kernel_matrix_vectorized","gp_online_vect"],"filenum":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,1,null,null,null,null,1,null,null,null,1,null,null,null,1,null],"linenum":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,4,4,null,null,null,null,14,null,null,null,14,null,null,null,14,null],"memalloc":[15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.3107986450195,15.7371368408203,15.7371368408203,15.7371368408203,15.7371368408203,15.7371368408203,16.5488815307617,16.5488815307617,16.5488815307617,16.5488815307617,17.1024932861328,17.1024932861328,17.1024932861328,17.1024932861328],"meminc":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.426338195800781,0,0,0,0,0.811744689941406,0,0,0,0.553611755371094,0,0,0],"filename":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"gp_online_vect.R","gp_online_vect.R",null,null,null,null,"gp_online_vect.R",null,null,null,"gp_online_vect.R",null,null,null,"gp_online_vect.R",null]},"interval":10,"files":[{"filename":"gp_online_vect.R","content":"# uses vectorized kernel matrix\ngp_online_vect <- function(xtrain, ytrain, xtest, sigma_n, sigmasq){\n  # Find kernel matrix\n  K <- kernel_matrix_vectorized(xtrain, sigmasq) \n  # Cholesky factor for K + sigma^2 * I\n  L <- chol(K + sigma_n^2*diag(ntrain))  ## Upper triangular\n  alpha <- backsolve(L, forwardsolve(t(L), ytrain))\n  # Allocate memory first\n  gpmean <- rep(0, ntest)\n  gpvar  <- rep(0, ntest)\n  # Loop through all test rows of do online regression\n  for (row_ix in 1:nrow(xtest)){\n    # Find kernel evaluation against all training points\n    kstar <- kernel_matrix_vectorized(matrix(xtest[row_ix, ]), sigmasq, xtrain)\n    # GP mean for current test point\n    gpmean[row_ix] <- kstar %*% alpha\n    # GP variance for current test point\n    gpvar[row_ix] <- 1.0 - crossprod(forwardsolve(t(L), t(kstar)))  ## as SE(x*, x*) = 1.0\n  }\n  return(list(gpmean, gpvar))\n}","normpath":"/home/mauro/Documents/University/StatisticalComputingPortfolioWebsite/content/post/gp_online_vect.R"}],"prof_output":"/tmp/RtmpOLBSfd/file6d613fa5071f.prof","highlight":{"output":["^output\\$"],"gc":["^<GC>$"],"stacktrace":["^\\.\\.stacktraceo(n|ff)\\.\\.$"]},"split":"h"}},"evals":[],"jsHooks":[]}</script>
<p>We can see that this implementation uses much less memory and it’s much faster.</p>
</div>
<div id="completely-vectorized-implementation" class="section level3">
<h3>Completely vectorized implementation</h3>
<p>We can combine these ideas to obtain a much faster implementation.</p>
<pre class="r"><code>source(&quot;gp_completely_vectorized.R&quot;, keep.source = TRUE)
profvis(gp_completely_vectorized(xtrain, ytrain, xtest, sigma_n, sigmasq), interval=0.005)</code></pre>
<div id="htmlwidget-4" style="width:100%;height:600px;" class="profvis html-widget"></div>
<script type="application/json" data-for="htmlwidget-4">{"x":{"message":{"prof":{"time":[1,1,2,2],"depth":[2,1,2,1],"label":["kernel_matrix_vectorized","gp_completely_vectorized","kernel_matrix_vectorized","gp_completely_vectorized"],"filenum":[2,null,2,null],"linenum":[5,null,5,null],"memalloc":[23.0797271728516,23.0797271728516,24.9870834350586,24.9870834350586],"meminc":[0,0,0,0],"filename":["gp_completely_vectorized.R",null,"gp_completely_vectorized.R",null]},"interval":5,"files":[{"filename":"gp_completely_vectorized.R","content":"gp_completely_vectorized <- function(xtrain, ytrain, xtest, sigma_n, sigmasq){\n  # Find kernel matrices in vectorized form\n  K  <-  kernel_matrix_vectorized(xtrain, sigmasq) \n  Ks <-  kernel_matrix_vectorized(xtest,  sigmasq, xtrain)\n  Kss <- kernel_matrix_vectorized(xtest, sigmasq)\n  # Cholesky factorization\n  L <- chol(K + sigma_n^2*diag(ntrain))  ## Upper triangular\n  alpha <- backsolve(L, forwardsolve(t(L), ytrain))\n  # Solve by forward and backward substitution\n  gpmean <- Ks %*% alpha\n  gpvcov <- Kss - Ks %*% backsolve(L, forwardsolve(t(L), t(Ks)))\n  return(list(gpmean, gpvcov))\n}\n","normpath":"/home/mauro/Documents/University/StatisticalComputingPortfolioWebsite/content/post/gp_completely_vectorized.R"}],"prof_output":"/tmp/RtmpOLBSfd/file6d614b099a3b.prof","highlight":{"output":["^output\\$"],"gc":["^<GC>$"],"stacktrace":["^\\.\\.stacktraceo(n|ff)\\.\\.$"]},"split":"h"}},"evals":[],"jsHooks":[]}</script>
</div>
</div>
<div id="visualizations" class="section level2">
<h2>Visualizations</h2>
<div id="before-seeing-training-data" class="section level3">
<h3>Before seeing training data</h3>
<p>Before seeing the training data we only have the test data. The Gaussian Process will therefore predict random (smooth) functions with mean zero.</p>
<pre class="r"><code>Kss &lt;- kernel_matrix_vectorized(xtest, sigmasq)
# Sample nprior_samples Multivariate Normals with mean zero and variance-covariance
# being the kernel matrix
data.frame(x=xtest, t(mvrnorm(nprior_samples, rep(0, length=ntest), Kss))) %&gt;% 
  setNames(c(&quot;x&quot;, sprintf(&quot;Sample %s&quot;, 1:nprior_samples))) %&gt;% 
  gather(&quot;MVN Samples&quot;, &quot;Value&quot;, -x) %&gt;% 
  ggplot(aes(x=x, y=Value)) + 
    # Because diag(Kss) are all 1s. We use mean +\- 2*standard deviation
    geom_rect(xmin=-Inf, xmax=Inf, ymin=-2, ymax=2, fill=&quot;grey80&quot;) + 
    geom_line(aes(color=`MVN Samples`)) +
    geom_abline(slope=0.0, intercept=0.0, lty=2) + 
    scale_y_continuous(lim=c(-3, 3)) + 
    labs(title=paste(nprior_samples, &quot;MVN Samples before seeing the data&quot;)) + 
    theme(plot.title=element_text(hjust=0.5))</code></pre>
<p><img src="/post/2020-01-13-profiling-portfolio_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</div>
<div id="after-seeing-training-data" class="section level3">
<h3>After seeing training data</h3>
<p>We only need to find the predicted mean and the predicted variance.</p>
<pre class="r"><code># Get predicitons. To predict noisy data just add sigma_n^2*diag(ncol(xtest)) 
# to the covariance matrix as implemented in the script 
# `gp_completely_vectorized_noisy.R`.
results &lt;- gp_completely_vectorized(xtrain, ytrain, xtest, sigma_n, sigmasq)

gpmean &lt;- results[[1]]
gpvcov &lt;- results[[2]]
# for plotting
dftrain = data.frame(xtrain=xtrain, ytrain=ytrain)
# Plot
data.frame(x=xtest, t(mvrnorm(npost_samples, gpmean, gpvcov))) %&gt;% 
  setNames(c(&quot;x&quot;, sprintf(&quot;Sample %s&quot;, 1:npost_samples))) %&gt;% 
  mutate(ymin=gpmean-2*sqrt(diag(gpvcov)), ymax=gpmean+2*sqrt(diag(gpvcov)),
         gpmean=gpmean, ytrue=regression_function(xtest)) %&gt;% 
  gather(&quot;MVN Samples&quot;, &quot;Value&quot;, -x, -ymin, -ymax, -gpmean, -ytrue) %&gt;% 
  ggplot(aes(x=x, y=Value)) + 
    geom_ribbon(aes(ymin=ymin, ymax=ymax), fill=&quot;grey80&quot;) + 
    geom_line(aes(color=`MVN Samples`)) +
    geom_line(aes(y=gpmean), size=1) +
    geom_line(aes(y=ytrue), color=&quot;darkred&quot;, lty=2) + 
    geom_point(data=dftrain, aes(x=xtrain, y=ytrain), color=&quot;red&quot;) +
    ggtitle(paste(npost_samples, &quot;MVN Samples after seeing&quot;, ntrain, &quot;data points&quot;)) + 
    theme(plot.title=element_text(hjust=0.5))</code></pre>
<p><img src="/post/2020-01-13-profiling-portfolio_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
</div>
</div>
<div id="note-on-vectorized-kernel-matrix" class="section level1">
<h1>Note on Vectorized Kernel Matrix</h1>
<p>One might wonder why in the function <code>kernel_matrix_vectorized()</code> we fill the matrix <code>Ynorm</code> by row. Afterall <code>R</code> works with column-major storage so this should be inefficient. One can compare filling in a matrix by row versus filling it by column and then taking the transpose in different cases:</p>
<ul>
<li>Number of rows &lt; Number of columns</li>
</ul>
<pre class="r"><code>nrows &lt;- 100
ncols &lt;- 2000
microbenchmark(
  rowwise=matrix(0, nrows, ncols, byrow=TRUE),
  transpose=t(matrix(0, nrows, ncols))
  )</code></pre>
<pre><code>## Unit: microseconds
##       expr     min       lq     mean   median        uq      max neval
##    rowwise 305.611 331.4385 626.9444 346.8885  667.2005 5273.726   100
##  transpose 529.117 567.7145 938.6158 616.6685 1220.6405 3105.513   100</code></pre>
<ul>
<li>Number of rows = Number of columns</li>
</ul>
<pre class="r"><code>nrows &lt;- 2000
ncols &lt;- 2000
microbenchmark(
  rowwise=matrix(0, nrows, ncols, byrow=TRUE),
  transpose=t(matrix(0, nrows, ncols))
  )</code></pre>
<pre><code>## Unit: milliseconds
##       expr      min       lq     mean   median       uq       max neval
##    rowwise 16.63772 24.05133 26.38047 24.53757 28.15542  84.38269   100
##  transpose 22.80104 29.19387 38.35057 36.31004 40.85461 105.26555   100</code></pre>
<ul>
<li>Number of rows &gt; Number of columns</li>
</ul>
<pre class="r"><code>nrows &lt;- 2000
ncols &lt;- 100
microbenchmark(
  rowwise=matrix(0, nrows, ncols, byrow=TRUE),
  transpose=t(matrix(0, nrows, ncols))
  )</code></pre>
<pre><code>## Unit: microseconds
##       expr     min       lq     mean   median       uq      max neval
##    rowwise 375.446 382.8210 512.2860 394.2025 430.1485 4333.774   100
##  transpose 561.959 582.1305 729.5464 601.0420 635.8135 2962.686   100</code></pre>
<p>Generally, <code>byrow=TRUE</code> is faster.</p>
</div>
