---
title: Profiling Portfolio
author: Mauro Camara Escudero
date: '2020-01-13'
slug: profiling-portfolio
categories:
  - R
  - profiling
tags:
  - profiling
  - gaussian-processes
keywords:
  - tech
header-includes:
  - \usepackage{amsmath}
---

<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/jquery/jquery.min.js"></script>
<script src="/rmarkdown-libs/d3/d3.min.js"></script>
<link href="/rmarkdown-libs/profvis/profvis.css" rel="stylesheet" />
<script src="/rmarkdown-libs/profvis/profvis.js"></script>
<link href="/rmarkdown-libs/highlight/textmate.css" rel="stylesheet" />
<script src="/rmarkdown-libs/highlight/highlight.js"></script>
<script src="/rmarkdown-libs/profvis-binding/profvis.js"></script>


<div id="implementations" class="section level2">
<h2>Implementations</h2>
<div id="data-generation-and-problem-settings" class="section level3">
<h3>Data Generation and Problem Settings</h3>
<p>Import the necessary packages. The library <code>provfis</code> is a stochastic profiler and can be used to profile our code.</p>
<pre class="r"><code>library(profvis)
library(tidyverse)
library(MASS)
library(microbenchmark)</code></pre>
<p>First, let’s decide some parameters and settings such as the amount of training and testing data.</p>
<pre class="r"><code># Number of data points for plotting &amp; Bandwidth squared of the RBF kernel
ntest &lt;- 500
# Characteristic Length-scale for kernel
sigmasq &lt;- 1.0
# Number of training points, standard deviation of additive noise
ntrain &lt;- 10
sigma_n &lt;- 0.5
# Define number of samples for prior gp mvn and posterior gp mvn to take
nprior_samples &lt;- 5
npost_samples &lt;- 5</code></pre>
<p>Define some helper functions. In particular, the kernel function is a squared exponential. The bandwidth is computed as the median of all pairwise distances. We also define a matrix that applies the squared exponential to rows of two matrices, i.e. it computes the kernel matrix <span class="math inline">\(K(X, Y)\)</span>. for two matrices <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Finally, we also have a regression function which represents the true structure of the data. This is just a quintic polynomial for simplicity, but can be replaced by more complicated functions.</p>
<pre class="r"><code>squared_exponential &lt;- function(x, c, sigmasq){
  return(exp(-0.5*sum((x - c)^2) / sigmasq))
}
kernel_matrix &lt;- function(X, Xstar, sigmasq){
  # compute the kernel matrix
  K &lt;- apply(
    X=Xstar,
    MARGIN=1, 
    FUN=function(xstar_row) apply(
      X=X, 
      MARGIN=1, 
      FUN=squared_exponential, 
      xstar_row,
      sigmasq
      )
    )
  return(K)
}
regression_function &lt;- function(x){
    val &lt;- (x+5)*(x+2)*(x)*(x-4)*(x-3)/10 + 2
  return(val)
}</code></pre>
<p>Now let’s actually generate some data</p>
<pre class="r"><code>set.seed(12345)
# training data
xtrain &lt;- matrix(runif(ntrain, min=-5, max=5))
ytrain &lt;- regression_function(xtrain) + matrix(rnorm(ntrain, sd=sigma_n))
# testing data
xtest &lt;- matrix(seq(-5,5, len=ntest))</code></pre>
</div>
<div id="naive-vectorized-implementation" class="section level3">
<h3>Naive Vectorized Implementation</h3>
<p>The first implementation that we look at is naive in the sense that it basically blindly copies the operations. This means that we invert <span class="math inline">\(K + \sigma_n^2 I\)</span> directly.</p>
<pre class="r"><code>source(&quot;gp_naive.R&quot;, keep.source = TRUE)
profvis(result &lt;- gp_naive(xtrain, ytrain, xtest, sigma_n, sigmasq))</code></pre>
<div id="htmlwidget-1" style="width:100%;height:600px;" class="profvis html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"message":{"prof":{"time":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,6,6,7,7,7,7,7,8,8,8,8,8,9,9,9,9,9,10,10,10,10,10,11,11,11,11,11,12,12,12,12,12,12,12,12,13,13,13,13,13,14,14,14,14,14,15,15,15,15,15,16,16,16,16,16,17,17,17,17,17,18,18,18,18,18,18,19,19,19,19,19,20,20,20,20,20,21,21,21,21,21,21,22,22,22,22,22,23,23,23,23,23,24,24,24,24,24,25,25,25,25,25,25,26,26,26,26,26,26,27,27,27,27,27,27,28,28,28,28,28,29,29,29,29,29,30,30,30,30,30,31,31,31,31,31,32,32,32,32,32,33,33,33,33,33,33,34,34,34,34,34,34,34,34,35,35,35,35,35,36,36,36,36,36,37,37,37,37,37,37,38,38,38,38],"depth":[32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,6,5,4,3,2,1,8,7,6,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,6,5,4,3,2,1,4,3,2,1],"label":["cmpBuiltinArgs","h","tryInline","cmpCall","cmp","cmpBuiltinArgs","h","tryInline","cmpCall","cmp","cmpPrim2","h","tryInline","cmpCall","cmp","cmpSymbolAssign","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","genCode","cmpfun","doTryCatch","tryCatchOne","tryCatchList","tryCatch","compiler:::tryCmpfun","gp_naive","FUN","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","FUN","apply","FUN","apply","kernel_matrix","gp_naive","FUN","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","<GC>","sum","FUN","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","FUN","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","FUN","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","FUN","apply","FUN","apply","kernel_matrix","gp_naive","FUN","apply","FUN","apply","kernel_matrix","gp_naive","lengths","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","FUN","apply","FUN","apply","kernel_matrix","gp_naive","<GC>","sum","FUN","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","apply","FUN","apply","kernel_matrix","gp_naive","FUN","apply","FUN","apply","kernel_matrix","gp_naive","array","apply","kernel_matrix","gp_naive"],"filenum":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,2,null,null,null,null,2,null,null,null,null,null,2,null,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,null,2,null,null,null,null,null,2,null,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,null,2,null,null,null,null,null,null,null,2,null,null,null,null,2,null,null,null,null,2,null,null,null,null,null,2,null,null,null,2,null],"linenum":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,4,null,null,null,null,5,null,null,null,null,null,5,null,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,null,5,null,null,null,null,null,5,null,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,null,5,null,null,null,null,null,null,null,5,null,null,null,null,5,null,null,null,null,5,null,null,null,null,null,5,null,null,null,5,null],"memalloc":[15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.2894897460938,15.5796127319336,15.5796127319336,15.5796127319336,15.5796127319336,15.5796127319336,15.5796127319336,16.2762603759766,16.2762603759766,16.2762603759766,16.2762603759766,16.2762603759766,16.6202621459961,16.6202621459961,16.6202621459961,16.6202621459961,16.6202621459961,16.6202621459961,17.155647277832,17.155647277832,17.155647277832,17.155647277832,17.155647277832,17.155647277832,17.49951171875,17.49951171875,17.49951171875,17.49951171875,17.49951171875,12.6587142944336,12.6587142944336,12.6587142944336,12.6587142944336,12.6587142944336,13.1412658691406,13.1412658691406,13.1412658691406,13.1412658691406,13.1412658691406,13.7898254394531,13.7898254394531,13.7898254394531,13.7898254394531,13.7898254394531,14.2169418334961,14.2169418334961,14.2169418334961,14.2169418334961,14.2169418334961,14.8773803710938,14.8773803710938,14.8773803710938,14.8773803710938,14.8773803710938,15.0316543579102,15.0316543579102,15.0316543579102,15.0316543579102,15.0316543579102,15.0316543579102,15.0316543579102,15.0316543579102,13.3393325805664,13.3393325805664,13.3393325805664,13.3393325805664,13.3393325805664,13.8094635009766,13.8094635009766,13.8094635009766,13.8094635009766,13.8094635009766,14.527946472168,14.527946472168,14.527946472168,14.527946472168,14.527946472168,14.995002746582,14.995002746582,14.995002746582,14.995002746582,14.995002746582,13.3030014038086,13.3030014038086,13.3030014038086,13.3030014038086,13.3030014038086,13.8082733154297,13.8082733154297,13.8082733154297,13.8082733154297,13.8082733154297,13.8082733154297,14.4989318847656,14.4989318847656,14.4989318847656,14.4989318847656,14.4989318847656,14.9696044921875,14.9696044921875,14.9696044921875,14.9696044921875,14.9696044921875,13.2904205322266,13.2904205322266,13.2904205322266,13.2904205322266,13.2904205322266,13.2904205322266,13.7920455932617,13.7920455932617,13.7920455932617,13.7920455932617,13.7920455932617,14.5355453491211,14.5355453491211,14.5355453491211,14.5355453491211,14.5355453491211,15.0342788696289,15.0342788696289,15.0342788696289,15.0342788696289,15.0342788696289,15.7674179077148,15.7674179077148,15.7674179077148,15.7674179077148,15.7674179077148,15.7674179077148,13.8568267822266,13.8568267822266,13.8568267822266,13.8568267822266,13.8568267822266,13.8568267822266,14.6131134033203,14.6131134033203,14.6131134033203,14.6131134033203,14.6131134033203,14.6131134033203,15.1044998168945,15.1044998168945,15.1044998168945,15.1044998168945,15.1044998168945,15.84326171875,15.84326171875,15.84326171875,15.84326171875,15.84326171875,13.9034423828125,13.9034423828125,13.9034423828125,13.9034423828125,13.9034423828125,14.6751861572266,14.6751861572266,14.6751861572266,14.6751861572266,14.6751861572266,15.1712875366211,15.1712875366211,15.1712875366211,15.1712875366211,15.1712875366211,15.8993911743164,15.8993911743164,15.8993911743164,15.8993911743164,15.8993911743164,15.8993911743164,16.3367385864258,16.3367385864258,16.3367385864258,16.3367385864258,16.3367385864258,16.3367385864258,16.3367385864258,16.3367385864258,14.7028884887695,14.7028884887695,14.7028884887695,14.7028884887695,14.7028884887695,15.2058563232422,15.2058563232422,15.2058563232422,15.2058563232422,15.2058563232422,15.9345474243164,15.9345474243164,15.9345474243164,15.9345474243164,15.9345474243164,15.9345474243164,20.1551971435547,20.1551971435547,20.1551971435547,20.1551971435547],"meminc":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.696647644042969,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.343864440917969,0,0,0,0,-4.84079742431641,0,0,0,0,0.482551574707031,0,0,0,0,0.6485595703125,0,0,0,0,0.427116394042969,0,0,0,0,0.660438537597656,0,0,0,0,0.154273986816406,0,0,0,0,0,0,0,-1.69232177734375,0,0,0,0,0.470130920410156,0,0,0,0,0.718482971191406,0,0,0,0,0.467056274414062,0,0,0,0,-1.69200134277344,0,0,0,0,0,0,0,0,0,0,0.690658569335938,0,0,0,0,0.470672607421875,0,0,0,0,0,0,0,0,0,0,0.501625061035156,0,0,0,0,0.743499755859375,0,0,0,0,0.498733520507812,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.75628662109375,0,0,0,0,0,0.491386413574219,0,0,0,0,0.738761901855469,0,0,0,0,-1.9398193359375,0,0,0,0,0.771743774414062,0,0,0,0,0.496101379394531,0,0,0,0,0,0,0,0,0,0,0.437347412109375,0,0,0,0,0,0,0,-1.63385009765625,0,0,0,0,0.502967834472656,0,0,0,0,0,0,0,0,0,0,4.22064971923828,0,0,0],"filename":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,null,"gp_naive.R",null,null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,null,"gp_naive.R",null,null,null,null,null,"gp_naive.R",null,null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,null,"gp_naive.R",null,null,null,null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,"gp_naive.R",null,null,null,null,null,"gp_naive.R",null,null,null,"gp_naive.R",null]},"interval":10,"files":[{"filename":"gp_naive.R","content":"gp_naive <- function(xtrain, ytrain, xtest, sigma_n, sigmasq){\n  # Find kernel matrices in vectorized form\n  K  <-  kernel_matrix(xtrain, xtrain, sigmasq) \n  Ks <-  kernel_matrix(xtest,  xtrain, sigmasq)\n  Kss <- kernel_matrix(xtest,  xtest, sigmasq)\n  # Find the inverse of K + sigma_n^2I directly\n  inverse <- solve(K + sigma_n^2 * diag(ntrain))\n  # GP mean\n  gpmean <- Ks %*% (inverse %*% ytrain)\n  # GP variance-covariance matrix\n  gpvcov <- Kss - Ks %*% inverse %*% t(Ks)\n  return(list(gpmean, gpvcov))\n}\n","normpath":"/home/mauro/Documents/University/StatisticalComputingPortfolioWebsite/content/post/gp_naive.R"}],"prof_output":"/tmp/Rtmpp1WwOn/file7b5f72732b6.prof","highlight":{"output":["^output\\$"],"gc":["^<GC>$"],"stacktrace":["^\\.\\.stacktraceo(n|ff)\\.\\.$"]},"split":"h"}},"evals":[],"jsHooks":[]}</script>
</div>
<div id="online-non-vectorized-cholesky-implementation" class="section level3">
<h3>Online Non-Vectorized Cholesky Implementation</h3>
<p>This implementation can also be used <em>online</em>. It is recommended in the “Gaussian Processes for Machine Learning” book.</p>
<pre class="r"><code>source(&quot;gp_online.R&quot;, keep.source = TRUE)
profvis(result_online &lt;- gp_online(xtrain, ytrain, xtest, sigma_n, sigmasq))</code></pre>
<div id="htmlwidget-2" style="width:100%;height:600px;" class="profvis html-widget"></div>
<script type="application/json" data-for="htmlwidget-2">{"x":{"message":{"prof":{"time":[1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,3,4,4,4,5,5,5,5,5,5,6,6,6,6,6,6,7,7,7,7,7,7,8,8,8,8,8,8,9,9,9,9,9,10,10,10,10,10,10],"depth":[6,5,4,3,2,1,5,4,3,2,1,6,5,4,3,2,1,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,6,5,4,3,2,1],"label":["aperm","apply","FUN","apply","kernel_matrix","gp_online","apply","FUN","apply","kernel_matrix","gp_online","aperm","apply","FUN","apply","kernel_matrix","gp_online","gpmean[row_ix] <- t(kstar) %*% alpha","t","gp_online","aperm","apply","FUN","apply","kernel_matrix","gp_online","aperm","apply","FUN","apply","kernel_matrix","gp_online","match.fun","apply","FUN","apply","kernel_matrix","gp_online","aperm","apply","FUN","apply","kernel_matrix","gp_online","apply","FUN","apply","kernel_matrix","gp_online","match.fun","apply","FUN","apply","kernel_matrix","gp_online"],"filenum":[null,null,null,null,2,null,null,null,null,2,null,null,null,null,null,2,null,2,2,null,null,null,null,null,2,null,null,null,null,null,2,null,null,null,null,null,2,null,null,null,null,null,2,null,null,null,null,2,null,null,null,null,null,2,null],"linenum":[null,null,null,null,13,null,null,null,null,13,null,null,null,null,null,13,null,15,15,null,null,null,null,null,13,null,null,null,null,null,13,null,null,null,null,null,13,null,null,null,null,null,13,null,null,null,null,13,null,null,null,null,null,13,null],"memalloc":[15.5103988647461,15.5103988647461,15.5103988647461,15.5103988647461,15.5103988647461,15.5103988647461,15.8665924072266,15.8665924072266,15.8665924072266,15.8665924072266,15.8665924072266,16.3895950317383,16.3895950317383,16.3895950317383,16.3895950317383,16.3895950317383,16.3895950317383,16.7129898071289,16.7129898071289,16.7129898071289,17.2167129516602,17.2167129516602,17.2167129516602,17.2167129516602,17.2167129516602,17.2167129516602,14.975212097168,14.975212097168,14.975212097168,14.975212097168,14.975212097168,14.975212097168,15.5079574584961,15.5079574584961,15.5079574584961,15.5079574584961,15.5079574584961,15.5079574584961,15.850959777832,15.850959777832,15.850959777832,15.850959777832,15.850959777832,15.850959777832,16.350944519043,16.350944519043,16.350944519043,16.350944519043,16.350944519043,16.702995300293,16.702995300293,16.702995300293,16.702995300293,16.702995300293,16.702995300293],"meminc":[0,0,0,0,0,0,0.356193542480469,0,0,0,0,0.523002624511719,0,0,0,0,0,0.323394775390625,0,0,0.50372314453125,0,0,0,0,0,-2.24150085449219,0,0,0,0,0,0,0,0,0,0,0,0.343002319335938,0,0,0,0,0,0.499984741210938,0,0,0,0,0.35205078125,0,0,0,0,0],"filename":[null,null,null,null,"gp_online.R",null,null,null,null,"gp_online.R",null,null,null,null,null,"gp_online.R",null,"gp_online.R","gp_online.R",null,null,null,null,null,"gp_online.R",null,null,null,null,null,"gp_online.R",null,null,null,null,null,"gp_online.R",null,null,null,null,null,"gp_online.R",null,null,null,null,"gp_online.R",null,null,null,null,null,"gp_online.R",null]},"interval":10,"files":[{"filename":"gp_online.R","content":"gp_online <- function(xtrain, ytrain, xtest, sigma_n, sigmasq){\n  # Find kernel matrix\n  K <- kernel_matrix(xtrain, xtrain, sigmasq) \n  # Cholesky factor for K + sigma^2 * I\n  L <- chol(K + sigma_n^2*diag(ntrain))  ## Upper triangular\n  alpha <- backsolve(L, forwardsolve(t(L), ytrain))\n  # Allocate memory first\n  gpmean <- rep(0, ntest)\n  gpvar  <- rep(0, ntest)\n  # Loop through all test rows of do online regression\n  for (row_ix in 1:nrow(xtest)){\n    # Find kernel evaluation against all training points\n    kstar <- kernel_matrix(matrix(xtest[row_ix, ]), xtrain, sigmasq)\n    # GP mean for current test point\n    gpmean[row_ix] <- t(kstar) %*% alpha\n    # GP variance for current test point\n    gpvar[row_ix] <- 1.0 - crossprod(forwardsolve(t(L), kstar))  ## as SE(x*, x*) = 1.0\n  }\n  return(list(gpmean, gpvar))\n}","normpath":"/home/mauro/Documents/University/StatisticalComputingPortfolioWebsite/content/post/gp_online.R"}],"prof_output":"/tmp/Rtmpp1WwOn/file7b5f57c9d20a.prof","highlight":{"output":["^output\\$"],"gc":["^<GC>$"],"stacktrace":["^\\.\\.stacktraceo(n|ff)\\.\\.$"]},"split":"h"}},"evals":[],"jsHooks":[]}</script>
</div>
<div id="online-vectorized-kernel-cholesky-implementation" class="section level3">
<h3>Online Vectorized-Kernel Cholesky Implementation</h3>
<p>We can see that most of the time is spent computing the kernel matrix. We can therefore find a faster way to compute it as follows</p>
<pre class="r"><code>kernel_matrix_vectorized &lt;- function(X, sigmasq, Y=NULL){
  if (is.null(Y)){
    Y &lt;- X
  }
  n &lt;- nrow(X)
  m &lt;- nrow(Y)
  # Find three matrices above
  Xnorm &lt;- matrix(apply(X^2, 1, sum), n, m)
  Ynorm &lt;- matrix(apply(Y^2, 1, sum), n, m, byrow=TRUE)
  XY &lt;- tcrossprod(X, Y)
  return(exp(-(Xnorm - 2*XY + Ynorm) / (2*sigmasq)))
}</code></pre>
<p>using this, we get</p>
<pre class="r"><code>source(&quot;gp_online_vect.R&quot;, keep.source = TRUE)
profvis(gp_online_vect(xtrain, ytrain, xtest, sigma_n, sigmasq))</code></pre>
<div id="htmlwidget-3" style="width:100%;height:600px;" class="profvis html-widget"></div>
<script type="application/json" data-for="htmlwidget-3">{"x":{"message":{"prof":{"time":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,4,4],"depth":[21,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,4,3,2,1,2,1],"label":["cmpCallSymFun","cmpCall","cmp","cmpSymbolAssign","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","genCode","cmpfun","doTryCatch","tryCatchOne","tryCatchList","tryCatch","compiler:::tryCmpfun","kernel_matrix_vectorized","gp_online_vect","aperm","apply","matrix","kernel_matrix_vectorized","gp_online_vect","apply","matrix","kernel_matrix_vectorized","gp_online_vect","crossprod","gp_online_vect"],"filenum":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,1,null,null,null,null,1,null,null,null,1,null,1,null],"linenum":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,4,4,null,null,null,null,14,null,null,null,14,null,18,null],"memalloc":[15.2971420288086,15.2971420288086,15.2971420288086,15.2971420288086,15.2971420288086,15.2971420288086,15.2971420288086,15.2971420288086,15.2971420288086,15.2971420288086,15.2971420288086,15.2971420288086,15.2971420288086,15.2971420288086,15.2971420288086,15.2971420288086,15.2971420288086,15.2971420288086,15.2971420288086,15.2971420288086,15.2971420288086,15.6446990966797,15.6446990966797,15.6446990966797,15.6446990966797,15.6446990966797,16.388069152832,16.388069152832,16.388069152832,16.388069152832,16.9076919555664,16.9076919555664],"meminc":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.347557067871094,0,0,0,0,0.743370056152344,0,0,0,0.519622802734375,0],"filename":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"gp_online_vect.R","gp_online_vect.R",null,null,null,null,"gp_online_vect.R",null,null,null,"gp_online_vect.R",null,"gp_online_vect.R",null]},"interval":10,"files":[{"filename":"gp_online_vect.R","content":"# uses vectorized kernel matrix\ngp_online_vect <- function(xtrain, ytrain, xtest, sigma_n, sigmasq){\n  # Find kernel matrix\n  K <- kernel_matrix_vectorized(xtrain, sigmasq) \n  # Cholesky factor for K + sigma^2 * I\n  L <- chol(K + sigma_n^2*diag(ntrain))  ## Upper triangular\n  alpha <- backsolve(L, forwardsolve(t(L), ytrain))\n  # Allocate memory first\n  gpmean <- rep(0, ntest)\n  gpvar  <- rep(0, ntest)\n  # Loop through all test rows of do online regression\n  for (row_ix in 1:nrow(xtest)){\n    # Find kernel evaluation against all training points\n    kstar <- kernel_matrix_vectorized(matrix(xtest[row_ix, ]), sigmasq, xtrain)\n    # GP mean for current test point\n    gpmean[row_ix] <- kstar %*% alpha\n    # GP variance for current test point\n    gpvar[row_ix] <- 1.0 - crossprod(forwardsolve(t(L), t(kstar)))  ## as SE(x*, x*) = 1.0\n  }\n  return(list(gpmean, gpvar))\n}","normpath":"/home/mauro/Documents/University/StatisticalComputingPortfolioWebsite/content/post/gp_online_vect.R"}],"prof_output":"/tmp/Rtmpp1WwOn/file7b5f465dcf9c.prof","highlight":{"output":["^output\\$"],"gc":["^<GC>$"],"stacktrace":["^\\.\\.stacktraceo(n|ff)\\.\\.$"]},"split":"h"}},"evals":[],"jsHooks":[]}</script>
<p>We can see that this implementation uses much less memory and it’s much faster.</p>
</div>
<div id="completely-vectorized-implementation" class="section level3">
<h3>Completely vectorized implementation</h3>
<p>We can combine these ideas to obtain a much faster implementation.</p>
<pre class="r"><code>source(&quot;gp_completely_vectorized.R&quot;, keep.source = TRUE)
profvis(gp_completely_vectorized(xtrain, ytrain, xtest, sigma_n, sigmasq), interval=0.005)</code></pre>
<div id="htmlwidget-4" style="width:100%;height:600px;" class="profvis html-widget"></div>
<script type="application/json" data-for="htmlwidget-4">{"x":{"message":{"prof":{"time":[1,1,2,2],"depth":[2,1,2,1],"label":["kernel_matrix_vectorized","gp_completely_vectorized","%*%","gp_completely_vectorized"],"filenum":[2,null,2,null],"linenum":[5,null,11,null],"memalloc":[24.9865570068359,24.9865570068359,27.0158615112305,27.0158615112305],"meminc":[0,0,2.02930450439453,0],"filename":["gp_completely_vectorized.R",null,"gp_completely_vectorized.R",null]},"interval":5,"files":[{"filename":"gp_completely_vectorized.R","content":"gp_completely_vectorized <- function(xtrain, ytrain, xtest, sigma_n, sigmasq){\n  # Find kernel matrices in vectorized form\n  K  <-  kernel_matrix_vectorized(xtrain, sigmasq) \n  Ks <-  kernel_matrix_vectorized(xtest,  sigmasq, xtrain)\n  Kss <- kernel_matrix_vectorized(xtest, sigmasq)\n  # Cholesky factorization\n  L <- chol(K + sigma_n^2*diag(ntrain))  ## Upper triangular\n  alpha <- backsolve(L, forwardsolve(t(L), ytrain))\n  # Solve by forward and backward substitution\n  gpmean <- Ks %*% alpha\n  gpvcov <- Kss - Ks %*% backsolve(L, forwardsolve(t(L), t(Ks)))\n  return(list(gpmean, gpvcov))\n}\n","normpath":"/home/mauro/Documents/University/StatisticalComputingPortfolioWebsite/content/post/gp_completely_vectorized.R"}],"prof_output":"/tmp/Rtmpp1WwOn/file7b5f6454bd36.prof","highlight":{"output":["^output\\$"],"gc":["^<GC>$"],"stacktrace":["^\\.\\.stacktraceo(n|ff)\\.\\.$"]},"split":"h"}},"evals":[],"jsHooks":[]}</script>
</div>
</div>
<div id="visualizations" class="section level2">
<h2>Visualizations</h2>
<div id="before-seeing-training-data" class="section level3">
<h3>Before seeing training data</h3>
<p>Before seeing the training data we only have the test data. The Gaussian Process will therefore predict random (smooth) functions with mean zero.</p>
<pre class="r"><code>Kss &lt;- kernel_matrix_vectorized(xtest, sigmasq)
# Sample nprior_samples Multivariate Normals with mean zero and variance-covariance
# being the kernel matrix
data.frame(x=xtest, t(mvrnorm(nprior_samples, rep(0, length=ntest), Kss))) %&gt;% 
  setNames(c(&quot;x&quot;, sprintf(&quot;Sample %s&quot;, 1:nprior_samples))) %&gt;% 
  gather(&quot;MVN Samples&quot;, &quot;Value&quot;, -x) %&gt;% 
  ggplot(aes(x=x, y=Value)) + 
    # Because diag(Kss) are all 1s. We use mean +\- 2*standard deviation
    geom_rect(xmin=-Inf, xmax=Inf, ymin=-2, ymax=2, fill=&quot;grey80&quot;) + 
    geom_line(aes(color=`MVN Samples`)) +
    geom_abline(slope=0.0, intercept=0.0, lty=2) + 
    scale_y_continuous(lim=c(-3, 3)) + 
    labs(title=paste(nprior_samples, &quot;MVN Samples before seeing the data&quot;)) + 
    theme(plot.title=element_text(hjust=0.5))</code></pre>
<p><img src="/post/2020-01-13-profiling-portfolio_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</div>
<div id="after-seeing-training-data" class="section level3">
<h3>After seeing training data</h3>
<p>We only need to find the predicted mean and the predicted variance.</p>
<pre class="r"><code># Get predicitons. To predict noisy data just add sigma_n^2*diag(ncol(xtest)) 
# to the covariance matrix as implemented in the script 
results &lt;- gp_completely_vectorized(xtrain, ytrain, xtest, sigma_n, sigmasq)

gpmean &lt;- results[[1]]
gpvcov &lt;- results[[2]]
# for plotting
dftrain = data.frame(xtrain=xtrain, ytrain=ytrain)
# Plot
data.frame(x=xtest, t(mvrnorm(npost_samples, gpmean, gpvcov))) %&gt;% 
  setNames(c(&quot;x&quot;, sprintf(&quot;Sample %s&quot;, 1:npost_samples))) %&gt;% 
  mutate(ymin=gpmean-2*sqrt(diag(gpvcov)), ymax=gpmean+2*sqrt(diag(gpvcov)),
         gpmean=gpmean, ytrue=regression_function(xtest)) %&gt;% 
  gather(&quot;MVN Samples&quot;, &quot;Value&quot;, -x, -ymin, -ymax, -gpmean, -ytrue) %&gt;% 
  ggplot(aes(x=x, y=Value)) + 
    geom_ribbon(aes(ymin=ymin, ymax=ymax), fill=&quot;grey80&quot;) + 
    geom_line(aes(color=`MVN Samples`)) +
    geom_line(aes(y=gpmean), size=1) +
    geom_line(aes(y=ytrue), color=&quot;darkred&quot;, lty=2) + 
    geom_point(data=dftrain, aes(x=xtrain, y=ytrain), color=&quot;red&quot;) +
    ggtitle(paste(npost_samples, &quot;MVN Samples after seeing&quot;, ntrain, &quot;data points&quot;)) + 
    theme(plot.title=element_text(hjust=0.5))</code></pre>
<p><img src="/post/2020-01-13-profiling-portfolio_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
</div>
</div>
<div id="note-on-vectorized-kernel-matrix" class="section level1">
<h1>Note on Vectorized Kernel Matrix</h1>
<p>One might wonder why in the function <code>kernel_matrix_vectorized()</code> we fill the matrix <code>Ynorm</code> by row. Afterall <code>R</code> works with column-major storage so this should be inefficient. One can compare filling in a matrix by row versus filling it by column and then taking the transpose in different cases:</p>
<ul>
<li>Number of rows &lt; Number of columns</li>
</ul>
<pre class="r"><code>nrows &lt;- 100
ncols &lt;- 2000
microbenchmark(
  rowwise=matrix(0, nrows, ncols, byrow=TRUE),
  transpose=t(matrix(0, nrows, ncols))
  )</code></pre>
<pre><code>## Unit: microseconds
##       expr     min       lq     mean   median        uq      max neval
##    rowwise 321.202 343.0085 648.0742 360.0345  697.1185 5734.401   100
##  transpose 534.310 569.7350 950.3921 608.9455 1233.3545 3354.230   100</code></pre>
<ul>
<li>Number of rows = Number of columns</li>
</ul>
<pre class="r"><code>nrows &lt;- 2000
ncols &lt;- 2000
microbenchmark(
  rowwise=matrix(0, nrows, ncols, byrow=TRUE),
  transpose=t(matrix(0, nrows, ncols))
  )</code></pre>
<pre><code>## Unit: milliseconds
##       expr      min       lq     mean   median       uq      max neval
##    rowwise 16.69890 24.08596 26.52232 24.48049 28.26676 91.81093   100
##  transpose 23.32213 29.73081 38.70225 36.65087 41.26867 96.43356   100</code></pre>
<ul>
<li>Number of rows &gt; Number of columns</li>
</ul>
<pre class="r"><code>nrows &lt;- 2000
ncols &lt;- 100
microbenchmark(
  rowwise=matrix(0, nrows, ncols, byrow=TRUE),
  transpose=t(matrix(0, nrows, ncols))
  )</code></pre>
<pre><code>## Unit: microseconds
##       expr     min      lq     mean   median       uq      max neval
##    rowwise 378.090 383.898 470.1237 392.2410 398.4715 2512.430   100
##  transpose 564.521 579.105 688.0039 593.8405 621.8380 2687.371   100</code></pre>
<p>Generally, <code>byrow=TRUE</code> is faster.</p>
</div>
